With the advent of intelligent speech assistants such as Apple's Siri, Microsoft's Cortana and Amazon's Alexa into the daily life of numerous people, natural language processing/generation and its applications seem to have manifested themselves to stay and further shape the way humans cooperate with software. To interact with a voice-activated interface demonstrates to most users just how far the technological basis behind these applications has advanced and exhibits the capacity of algorithms to handle natural language. These bots may represent the pinnacle of current advancements to the layperson, though most of the underlying subdivisions of their software are based on tasks the fields of natural language processing, natural language generation and machine learning have been dealing with for decades. 

This thesis will have its focus on one important pre-processing task relevant to a wide variety of natural language processing applications, the automatic categorization of words in classes describing their grammatical behavior, called Part-of-Speech-tagging.

Annotating corpora with Part-of-Speech-tags has been done manually in the past \citep{martinez2012part}. One famous example of an annotated corpus, consisting of roughly one million words, is the Brown Corpus. In 1967 Henry Kucera and W. Nelson Francis compiled texts selected from a wide scope of sources of American English \citep{kucera1967brown}. This resulted in a corpus that has been cited extensively, because of its variegated nature and has been tagged partly with rule-based systems \citep{schmid1994part} and manually in the years following its compilation. Among others this annotated corpora helped to develop taggers with an accuracy of up to 97\% \citep{jurafsky2021}. Hence for the English language humans tend to perform the task of Part-of-Speech-tagging as well as the current state-of-the-art algorithms \citep{manning2011part}.

Two major difficulties for the successful completion of this task are the ambiguity of words and the encounter of before unseen words. Consider the example sentence of \cite{martinez2012part} 'We can can the can'. Three different syntactic functions of the word 'can' are represented in these five words. In order from left to right, 'can' is first an auxiliary, a verb and finally a noun, thereby showcasing the ambiguity of this word in one very short sentence.  

This thesis will first outline the general framework of Part-of-Speech-tagging. Afterwards an exploration of the theoretical foundation of LSTM (Long Short-Term Memory) Neural Networks will be given which will touch on the basic concepts of Feed-Forward and Recurrent Neural Networks as they build the foundation for the model used as the Part-of-Speech-tagger. Next a short overview will demonstrate some of the different possibilities to encode the semantic representation of a word into an embedding, thereby concluding the theoretical part of this work.

A short description of the Georgetown University Multilayer Corpus (GUM) will be provided since this annotated corpus is used for the empiric analysis. 
Following this the implementation of the feature selection process and the setup of the architecture for  the Part-of-Speech-tagging model will be examined. Finally the utility of the different linguistic features, provided as inputs to the tagger, will be evaluated.
