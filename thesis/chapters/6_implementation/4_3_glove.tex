As there exists no module to implement the GloVe framework to train word embeddings on specific data in Python, pre-trained word embeddings from \citet{pennington2014glove} were utilized. They were originally trained on a corpus of 6 billion words compiled from Wikipedia and the English Gigaword archive.

The pre-trained word embeddings come with a vector size of 50, 100, 200 and 300. To maintain a valid comparability between the different word embedding frameworks only the 50, 100 and 200 dimensional ones were considered. As for the other two frameworks, these three different GloVe word embeddings were tested on their performance for the POS-tagging task on the training data by the accuracy of a LSTM Neural Network with the word embeddings as their sole input with the two different numbers of units in the hidden layer (75, 125).
