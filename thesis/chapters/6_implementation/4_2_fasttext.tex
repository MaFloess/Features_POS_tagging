As mentioned in chapter \ref{word_fasttext}, the FastText framework was developed as an extension to Word2Vec. Therefore it was deemed reasonable to build different FastText models with the hyperparameters of the best performing Word2Vec word embeddings and thereby limit the combinations of parameters for which FastText word embeddings are created.

As mentioned in the last paragraph of chapter \ref{word2vec_imp}, each Word2Vec model has been evaluated with two different LSTM Neural Networks which differ only in their number of units in the hidden layer. The accuracy of these models decides which word embeddings have performed best on the POS-tagging task.

In this thesis the five best performing Word2Vec word embeddings will provide most of the hyperparameters for the FastText models and are combined with two additional hyperparameters which are inherent to the FastText framework. FastText models incorporate sub-word information by considering character ngrams. This is done by defining the range of character ngrams that should be embedded using the hyperparameters, minimum ($min\_n$) and maximum ($max\_n$) character sequence length. While the $gensim.models.Fasttext$ function incorporates the minimum and maximum as separate parameters, they were treated as a tuple in this thesis. The ranges used for training the FastText word embeddings were (2, 5) and (3, 6).

This resulted in 10 different FastText word embeddings, 5 different sets of hyperparameters of the best performing Word2Vec models times the 2 different characters ranges for character ngrams. These were evaluated in the same way as described in chapter \ref{word2vec_imp} and use the same $unit\_count$(s) as the Word2Vec models on which they are based.