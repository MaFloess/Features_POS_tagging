As mentioned at the end of the previous chapter, only about a third of the POS-tagging models beat the baseline tagger with regards to the accuracy on the GUM data. This begs the question what limitations of the self-build models may have caused such an outcome.

Firstly, the self-traind  word embeddings need to be considered. As mentioned by \citet{mikolov2013efficient} and \citet{jain2016fasttext} these frameworks were originally trained on huge datasets while the training data for GUM is approximately 4 orders of magnitude smaller in the case of the Word2Vec embeddings. Thereby the quality of the self-trained word embeddings may be put to question. Especially since the pre-trained GloVe embeddings provide mcuh better results if only word embeddings are utilized in the POS-tagging model.

Additionally the size of the used dataset may not be enough to let the training procedure of the LSTM Neural Network perfectly capture the regularities in the data.

As the computing resources for this thesis were limited, the limited data size was favorable, but if this evaluation is to be repeated with more computing power greater amounts of data and more tuning of hyperparameters for the word embeddings and models may result in POS-tagging models that perform better.

The way in which affixes were included as features for the POS-tagging model may also be put to question. Prefixes are searched for by recognizing patterns which resemble the prefixes of the provided list of prefixes. While this may catch all of the prefixes present in the data and the list, it also encodes integral parts of words that are no prefixes as such. One example is the word 'read' which has no prefix, but the particular way in which prefixes are searched for in this thesis encodes the prefix 're' for this word as the pattern of characters is found at the beginning of the word.
Similarly the way in which suffixes are retained by utilizing a stemmer is at the mercy of the stemmers accuracy in performing its task. It is known that stemmers in the NLTK package are efficient though often error prone since they may wrongly cut of characters at the end of a word that are not suffixes.
To ensure the the correctness of the encoding process for affixes, further analyses could implement lexical databases such as WordNet to check whether the word being stripped of its possible affixes is still a meaningful entity.

Another thing to consider is that LSTM Neural Networks are models to capture structural properties of data in one direction, moving forward through a sequence. Though syntactical relationships may comprise words that come at the end of a sequence of words, sentences. Therefore the inverse direction may hold valuable too, since words further down the sequence can encompass information relevant to the prediction of tags for words that come at a previous point in the sentence. If one wants to further explore the topic of this thesis, Bidirectional Recurrent Neural Networks could be explored as to ascertain whether they are more appropriate architectures for the POS-tagging model.
